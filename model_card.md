
# Model Card â€” Sentimovie: IMDb Duygu Analizi

**GeliÅŸtirici:** Emrullah GÃ¼nay  
**Versiyon:** 0.2.0  
**Son GÃ¼ncelleme:** 2024

## ğŸ“‹ Model Bilgileri

### Temel Bilgiler
- **Model AdÄ±:** Sentimovie IMDb Sentiment Classifier
- **Model TÃ¼rÃ¼:** Fine-tuned DistilBERT
- **Base Model:** `distilbert-base-uncased`
- **Task:** Binary Sentiment Classification (IMDb Movie Reviews)
- **Dil:** Ä°ngilizce
- **Model Boyutu:** ~260 MB

### Mimari
- **Architecture:** DistilBERT (Distilled BERT)
- **Layers:** 6 transformer layers
- **Hidden Size:** 768
- **Attention Heads:** 12
- **Parameters:** ~66M (trainable)

## ğŸ¯ KullanÄ±m AmacÄ±

### Ana AmaÃ§
IMDb film yorumlarÄ±nda pozitif/negatif duygu sÄ±nÄ±flandÄ±rmasÄ± yapmak.

### KullanÄ±m SenaryolarÄ±
- Film yorum analizi
- Sosyal medya sentiment analizi
- Customer feedback analizi
- Content moderation

### SÄ±nÄ±rlamalar
- Sadece Ä°ngilizce metinler
- Binary classification (pozitif/negatif)
- IMDb yorumlarÄ±na Ã¶zel eÄŸitim
- KÄ±sa metinler iÃ§in optimize edilmiÅŸ

## ğŸ“Š Performans Metrikleri

### Test SonuÃ§larÄ± (2% veri, 1 epoch)
| Metric | Value |
|--------|-------|
| Accuracy | 87.5% |
| F1 Macro | 87.3% |
| F1 Weighted | 87.8% |
| Precision Macro | 87.8% |
| Recall Macro | 87.1% |

### SÄ±nÄ±f BazÄ±nda Performans
- **Negatif SÄ±nÄ±f (0):** F1: 87.2%
- **Pozitif SÄ±nÄ±f (1):** F1: 87.4%

> **Not:** Bu metrikler kÃ¼Ã§Ã¼k veri Ã¶rneÄŸi Ã¼zerinde kÄ±sa eÄŸitim sÃ¼resi ile elde edilmiÅŸtir.

## ğŸ—‚ï¸ Veri KÃ¼mesi

### EÄŸitim Verisi
- **Kaynak:** IMDb Large Movie Review Dataset
- **Boyut:** 25,000 eÄŸitim Ã¶rneÄŸi (2% sample)
- **Format:** Text + Binary Label
- **Dil:** Ä°ngilizce

### Test Verisi
- **Boyut:** 25,000 test Ã¶rneÄŸi (2% sample)
- **Split:** Orijinal veri kÃ¼mesindeki ayrÄ±m

### Veri Ã–n Ä°ÅŸleme
- **Tokenization:** DistilBERT tokenizer
- **Max Length:** 128 tokens
- **Padding:** Max length padding
- **Truncation:** Long sequences truncated

## ğŸ—ï¸ EÄŸitim DetaylarÄ±

### Hiperparametreler
```yaml
Model: distilbert-base-uncased
Epochs: 1.0
Learning Rate: 5e-5
Batch Size: 8
Max Length: 128
Weight Decay: 0.01
Warmup Steps: 100
```

### EÄŸitim OrtamÄ±
- **Hardware:** CPU-only
- **Framework:** PyTorch + Transformers
- **Optimizer:** AdamW
- **Scheduler:** Linear warmup + decay
- **Loss Function:** CrossEntropyLoss

### EÄŸitim SÃ¼resi
- **2% veri, 1 epoch:** ~2-5 dakika (CPU)
- **Tam veri, 3 epochs:** ~2-4 saat (CPU)

## ğŸš€ KullanÄ±m

### API Endpoint
```bash
POST /predict
{
  "text": "This movie was absolutely fantastic!"
}
```

### Response Format
```json
{
  "label": 1,
  "probability": [0.123, 0.877],
  "confidence": 0.877,
  "text": "This movie was absolutely fantastic!"
}
```

### Batch Prediction
```bash
POST /batch-predict
[
  "Great movie!",
  "Terrible movie!",
  "Okay movie."
]
```

## ğŸ”§ Teknik Detaylar

### Model YÃ¼kleme
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("models/distilbert-imdb")
model = AutoModelForSequenceClassification.from_pretrained("models/distilbert-imdb")
```

### Inference
```python
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
with torch.no_grad():
    outputs = model(**inputs)
    probabilities = torch.softmax(outputs.logits, dim=-1)
    prediction = torch.argmax(outputs.logits, dim=-1)
```

### Memory Requirements
- **Model Loading:** ~260 MB RAM
- **Inference:** ~512 MB RAM (batch size 1)
- **Training:** ~2 GB RAM (batch size 8)

## ğŸ“ˆ Model VersiyonlarÄ±

### v0.1.0 (Base)
- Base DistilBERT model
- No fine-tuning
- Basic sentiment classification

### v0.2.0 (Current)
- Fine-tuned on IMDb data
- Improved accuracy
- Better handling of movie review language

## ğŸ” Model Analizi

### GÃ¼Ã§lÃ¼ YÃ¶nler
- HÄ±zlÄ± inference (CPU-friendly)
- KÃ¼Ã§Ã¼k model boyutu
- Ä°yi accuracy/performance ratio
- Stable predictions

### ZayÄ±f YÃ¶nler
- Limited to English
- Binary classification only
- Short text optimization
- Domain-specific (movie reviews)

### Bias ve Adalet
- IMDb veri kÃ¼mesindeki potansiyel bias'lar
- English-centric training data
- Movie review domain bias

## ğŸ›¡ï¸ GÃ¼venlik ve Etik

### Veri GizliliÄŸi
- Training data: Public IMDb dataset
- No personal information stored
- Model does not memorize training examples

### KullanÄ±m KÄ±sÄ±tlamalarÄ±
- Research and educational purposes
- Commercial use requires evaluation
- Not for critical decision making

### Potansiyel Riskler
- Misclassification of sarcasm
- Cultural bias in sentiment
- Overconfidence in predictions

## ğŸ“š Referanslar

### Papers
- [DistilBERT: Distilling BERT for NLP](https://arxiv.org/abs/1910.01108)
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

### Datasets
- [IMDb Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)

### Tools
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [PyTorch](https://pytorch.org/)

## ğŸ“ Ä°letiÅŸim

**Model GeliÅŸtirici:** Emrullah GÃ¼nay  
**GitHub:** [@emrullahgunay](https://github.com/emrullahgunay)  
**Email:** [emrullah@example.com](mailto:emrullah@example.com)

---

*Bu model card, modelin ÅŸeffaf ve sorumlu kullanÄ±mÄ±nÄ± desteklemek iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.*
